{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, unicode_literals, absolute_import, division\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "#To run the prediction on the GPU, else comment out this line to use the CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from csbdeep.utils.tf import limit_gpu_memory\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "sys.path.append('../../Terminator/')\n",
    "import glob\n",
    "from tifffile import imread\n",
    "\n",
    "from csbdeep.utils import Path, normalize\n",
    "from stardist import random_label_cmap, _draw_polygons,dist_to_coord\n",
    "from stardist.models import StarDist2D\n",
    "from TerminatorUtils.helpers import normalizeFloat, OtsuThreshold2D, save_8bit_tiff_imagej_compatible,Integer_to_border,SeedStarDistWatershed\n",
    "from TerminatorUtils.helpers import Prob_to_Binary\n",
    "from TerminatorUtils.helpers import save_tiff_imagej_compatible\n",
    "from csbdeep.models import Config, CARE\n",
    "np.random.seed(6)\n",
    "lbl_cmap = random_label_cmap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/media/sancere/Newton_Volume_1/wt_N10'\n",
    "\n",
    "basedirWaterResults = '/media/sancere/Newton_Volume_1/wt_N10/WaterResults/'\n",
    "basedirStarResults = '/media/sancere/Newton_Volume_1/wt_N10/StarResults/'\n",
    "basedirFinalResults = '/media/sancere/Newton_Volume_1/wt_N10/VarunFinalSolution/'\n",
    "basedirMaskResults = '/media/sancere/Newton_Volume_1/wt_N10/VarunMaskSolution/'\n",
    "\n",
    "Model_Dir = '/media/sancere/Newton_Volume_1/CurieDeepLearningModels/DrosophilaSegmentation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sancere/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sancere/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sancere/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sancere/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sancere/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "Loading network weights from 'weights_best.h5'.\n",
      "WARNING:tensorflow:From /home/sancere/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sancere/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sancere/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sancere/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sancere/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sancere/anaconda3/envs/tensorflowGPU/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Loading thresholds from 'thresholds.json'.\n",
      "Using default values: prob_thresh=0.494159, nms_thresh=0.3.\n"
     ]
    }
   ],
   "source": [
    "Path(basedirStarResults).mkdir(exist_ok = True)\n",
    "Path(basedirWaterResults).mkdir(exist_ok = True)\n",
    "Path(basedirFinalResults).mkdir(exist_ok = True)\n",
    "Path(basedirMaskResults).mkdir(exist_ok = True)\n",
    "\n",
    "StardistModelName = 'DrosophilaSegmentationSmartSeedsPatch256Ds2'\n",
    "\n",
    "model = StarDist2D(config = None, name = StardistModelName, basedir = Model_Dir)\n",
    "\n",
    "Raw_path = os.path.join(basedir, '*tif') #tif or TIF be careful\n",
    "\n",
    "\n",
    "axis_norm = (0,1)   # normalize channels independently\n",
    "axes = 'XY'\n",
    "\n",
    "filesRaw = glob.glob(Raw_path)\n",
    "# filesRaw.sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of seeds found: 6730\n",
      "Starting flooding\n",
      "Total number of seeds found: 7725\n",
      "Starting flooding\n",
      "Total number of seeds found: 8346\n",
      "Starting flooding\n",
      "Total number of seeds found: 8438\n",
      "Starting flooding\n",
      "Total number of seeds found: 8236\n",
      "Starting flooding\n",
      "Total number of seeds found: 6201\n",
      "Starting flooding\n",
      "Total number of seeds found: 9266\n",
      "Starting flooding\n",
      "Total number of seeds found: 6618\n",
      "Starting flooding\n",
      "Total number of seeds found: 6654\n",
      "Starting flooding\n",
      "Total number of seeds found: 7335\n",
      "Starting flooding\n",
      "Total number of seeds found: 7382\n",
      "Starting flooding\n",
      "Total number of seeds found: 7353\n",
      "Starting flooding\n",
      "Total number of seeds found: 7978\n",
      "Starting flooding\n",
      "Total number of seeds found: 8276\n",
      "Starting flooding\n",
      "Total number of seeds found: 8246\n",
      "Starting flooding\n",
      "Total number of seeds found: 7516\n",
      "Starting flooding\n",
      "Total number of seeds found: 9007\n",
      "Starting flooding\n",
      "Total number of seeds found: 8290\n",
      "Starting flooding\n",
      "Total number of seeds found: 6732\n",
      "Starting flooding\n",
      "Total number of seeds found: 7426\n",
      "Starting flooding\n",
      "Total number of seeds found: 7402\n",
      "Starting flooding\n",
      "Total number of seeds found: 6536\n",
      "Starting flooding\n",
      "Total number of seeds found: 6693\n",
      "Starting flooding\n",
      "Total number of seeds found: 6557\n",
      "Starting flooding\n",
      "Total number of seeds found: 6170\n",
      "Starting flooding\n",
      "Total number of seeds found: 6357\n",
      "Starting flooding\n",
      "Total number of seeds found: 4738\n",
      "Starting flooding\n",
      "Total number of seeds found: 7610\n",
      "Starting flooding\n",
      "Total number of seeds found: 7849\n",
      "Starting flooding\n",
      "Total number of seeds found: 8062\n",
      "Starting flooding\n",
      "Total number of seeds found: 8759\n",
      "Starting flooding\n",
      "Total number of seeds found: 6319\n",
      "Starting flooding\n",
      "Total number of seeds found: 9231\n",
      "Starting flooding\n",
      "Total number of seeds found: 6393\n",
      "Starting flooding\n",
      "Total number of seeds found: 9103\n",
      "Starting flooding\n",
      "Total number of seeds found: 8471\n",
      "Starting flooding\n",
      "Total number of seeds found: 8263\n",
      "Starting flooding\n",
      "Total number of seeds found: 8889\n",
      "Starting flooding\n",
      "Total number of seeds found: 6118\n",
      "Starting flooding\n",
      "Total number of seeds found: 4806\n",
      "Starting flooding\n",
      "Total number of seeds found: 4672\n",
      "Starting flooding\n",
      "Total number of seeds found: 9209\n",
      "Starting flooding\n",
      "Total number of seeds found: 8752\n",
      "Starting flooding\n",
      "Total number of seeds found: 6727\n",
      "Starting flooding\n",
      "Total number of seeds found: 7312\n",
      "Starting flooding\n",
      "Total number of seeds found: 4598\n",
      "Starting flooding\n",
      "Total number of seeds found: 4693\n",
      "Starting flooding\n",
      "Total number of seeds found: 7645\n",
      "Starting flooding\n",
      "Total number of seeds found: 7637\n",
      "Starting flooding\n",
      "Total number of seeds found: 9101\n",
      "Starting flooding\n",
      "Total number of seeds found: 7679\n",
      "Starting flooding\n",
      "Total number of seeds found: 6144\n",
      "Starting flooding\n",
      "Total number of seeds found: 8212\n",
      "Starting flooding\n",
      "Total number of seeds found: 8437\n",
      "Starting flooding\n",
      "Total number of seeds found: 6624\n",
      "Starting flooding\n",
      "Total number of seeds found: 9238\n",
      "Starting flooding\n",
      "Total number of seeds found: 6894\n",
      "Starting flooding\n",
      "Total number of seeds found: 8108\n",
      "Starting flooding\n",
      "Total number of seeds found: 7123\n",
      "Starting flooding\n",
      "Total number of seeds found: 8967\n",
      "Starting flooding\n",
      "Total number of seeds found: 6967\n",
      "Starting flooding\n",
      "Total number of seeds found: 8175\n",
      "Starting flooding\n",
      "Total number of seeds found: 6920\n",
      "Starting flooding\n",
      "Total number of seeds found: 7531\n",
      "Starting flooding\n",
      "Total number of seeds found: 5769\n",
      "Starting flooding\n",
      "Total number of seeds found: 6295\n",
      "Starting flooding\n",
      "Total number of seeds found: 9084\n",
      "Starting flooding\n",
      "Total number of seeds found: 6861\n",
      "Starting flooding\n",
      "Total number of seeds found: 4602\n",
      "Starting flooding\n",
      "Total number of seeds found: 7792\n",
      "Starting flooding\n",
      "Total number of seeds found: 7367\n",
      "Starting flooding\n",
      "Total number of seeds found: 9397\n",
      "Starting flooding\n",
      "Total number of seeds found: 7412\n",
      "Starting flooding\n",
      "Total number of seeds found: 8990\n",
      "Starting flooding\n",
      "Total number of seeds found: 6112\n",
      "Starting flooding\n",
      "Total number of seeds found: 7155\n",
      "Starting flooding\n",
      "Total number of seeds found: 7241\n",
      "Starting flooding\n",
      "Total number of seeds found: 4692\n",
      "Starting flooding\n",
      "Total number of seeds found: 9103\n",
      "Starting flooding\n",
      "Total number of seeds found: 8413\n",
      "Starting flooding\n",
      "Total number of seeds found: 7503\n",
      "Starting flooding\n",
      "Total number of seeds found: 6929\n",
      "Starting flooding\n",
      "Total number of seeds found: 9427\n",
      "Starting flooding\n",
      "Total number of seeds found: 6645\n",
      "Starting flooding\n",
      "Total number of seeds found: 7131\n",
      "Starting flooding\n",
      "Total number of seeds found: 4949\n",
      "Starting flooding\n",
      "Total number of seeds found: 6910\n",
      "Starting flooding\n",
      "Total number of seeds found: 5588\n",
      "Starting flooding\n",
      "Total number of seeds found: 8622\n",
      "Starting flooding\n",
      "Total number of seeds found: 6851\n",
      "Starting flooding\n",
      "Total number of seeds found: 7511\n",
      "Starting flooding\n",
      "Total number of seeds found: 7165\n",
      "Starting flooding\n",
      "Total number of seeds found: 9132\n",
      "Starting flooding\n",
      "Total number of seeds found: 7175\n",
      "Starting flooding\n",
      "Total number of seeds found: 7877\n",
      "Starting flooding\n",
      "Total number of seeds found: 9478\n",
      "Starting flooding\n",
      "Total number of seeds found: 8460\n",
      "Starting flooding\n",
      "Total number of seeds found: 4654\n",
      "Starting flooding\n",
      "Total number of seeds found: 8962\n",
      "Starting flooding\n",
      "Total number of seeds found: 8412\n",
      "Starting flooding\n",
      "Total number of seeds found: 7838\n",
      "Starting flooding\n",
      "Total number of seeds found: 7674\n",
      "Starting flooding\n",
      "Total number of seeds found: 7974\n",
      "Starting flooding\n",
      "Total number of seeds found: 9196\n",
      "Starting flooding\n",
      "Total number of seeds found: 8435\n",
      "Starting flooding\n",
      "Total number of seeds found: 7492\n",
      "Starting flooding\n",
      "Total number of seeds found: 9188\n",
      "Starting flooding\n",
      "Total number of seeds found: 8430\n",
      "Starting flooding\n",
      "Total number of seeds found: 7896\n",
      "Starting flooding\n",
      "Total number of seeds found: 7432\n",
      "Starting flooding\n",
      "Total number of seeds found: 9243\n",
      "Starting flooding\n",
      "Total number of seeds found: 6517\n",
      "Starting flooding\n",
      "Total number of seeds found: 8908\n",
      "Starting flooding\n",
      "Total number of seeds found: 7347\n",
      "Starting flooding\n",
      "Total number of seeds found: 8346\n",
      "Starting flooding\n",
      "Total number of seeds found: 9313\n",
      "Starting flooding\n",
      "Total number of seeds found: 8285\n",
      "Starting flooding\n",
      "Total number of seeds found: 8420\n",
      "Starting flooding\n",
      "Total number of seeds found: 6264\n",
      "Starting flooding\n",
      "Total number of seeds found: 6932\n",
      "Starting flooding\n",
      "Total number of seeds found: 7585\n",
      "Starting flooding\n",
      "Total number of seeds found: 7365\n",
      "Starting flooding\n",
      "Total number of seeds found: 9227\n",
      "Starting flooding\n",
      "Total number of seeds found: 7244\n",
      "Starting flooding\n",
      "Total number of seeds found: 7879\n",
      "Starting flooding\n",
      "Total number of seeds found: 7416\n",
      "Starting flooding\n",
      "Total number of seeds found: 6163\n",
      "Starting flooding\n",
      "Total number of seeds found: 4673\n",
      "Starting flooding\n"
     ]
    }
   ],
   "source": [
    "for fname in filesRaw:\n",
    "\n",
    "        if os.path.exists(basedirWaterResults + 'WaterResults_' + os.path.basename(os.path.splitext(fname)[0])) == False :\n",
    "            # Name = os.path.basename(os.path.splitext(fname)[0])\n",
    "            x = imread(fname)\n",
    "            x = normalizeFloat(x,1,99.8, axis = axis_norm)\n",
    "        \n",
    "            #Stardist, label image, details, probability map, distance map\n",
    "            segmentationImage, details = model.predict_instances(x)\n",
    "            prob, dist = model.predict(x)\n",
    "            grid = model.config.grid\n",
    "            prob = cv2.resize(prob, dsize=(prob.shape[1] * grid[1],prob.shape[0] * grid[0]-1))    # last change \n",
    "            Binary = Prob_to_Binary(prob, segmentationImage)\n",
    "            Mask = Prob_to_Binary(prob, segmentationImage) \n",
    "                \n",
    "                \n",
    "            #Seeds from Stardist, segmentation on probability map \n",
    "            Watershed, markers = SeedStarDistWatershed(prob,details['points'],model.config.grid )\n",
    "        \n",
    "            #Convert Integer image to binary\n",
    "            Binary_Watershed = Integer_to_border(Watershed)\n",
    "            Binary_Original_Star = Integer_to_border(segmentationImage)\n",
    "        \n",
    "            FinalImage = np.logical_and(Mask, Binary_Watershed)\n",
    "       \n",
    "            #Save different method segmentations\n",
    "            save_8bit_tiff_imagej_compatible((basedirWaterResults + 'WaterResults_' + os.path.basename(os.path.splitext(fname)[0])) , Binary_Watershed, axes)\n",
    "            save_8bit_tiff_imagej_compatible((basedirStarResults + 'StarResults_' + os.path.basename(os.path.splitext(fname)[0]) ) , Binary, axes)\n",
    "            save_8bit_tiff_imagej_compatible((basedirFinalResults + 'FinalResults_' + os.path.basename(os.path.splitext(fname)[0])) , Binary_Watershed, axes)\n",
    "            save_8bit_tiff_imagej_compatible((basedirMaskResults + 'MaskResults_' + os.path.basename(os.path.splitext(fname)[0]) ) , Binary, axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
