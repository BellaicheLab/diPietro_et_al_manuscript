{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount = True)\n",
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiffile\n",
    "!pip install elasticdeform\n",
    "!pip install keras==2.2.5\n",
    "!pip install csbdeep\n",
    "!pip install stardist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd '/content/drive/My Drive/BTrack/SegmentationTerminator/Terminator/'\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.util import invert\n",
    "%matplotlib inline\n",
    "from skimage import measure\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "sys.path.append('../../../Terminator/')\n",
    "import glob\n",
    "from csbdeep.utils import Path, normalize\n",
    "from tifffile import imread\n",
    "from stardist.models import StarDist2D\n",
    "from TerminatorUtils.helpers import normalizeFloat, OtsuThreshold2D, save_8bit_tiff_imagej_compatible,Integer_to_border, RelabelZ\n",
    "from TerminatorUtils.helpers import Prob_to_Binary, zero_pad, multiplot, CCLabels,doubleplot, remove_big_objects,fill_label_holes\n",
    "from TerminatorUtils.helpers import save_tiff_imagej_compatible, SeedStarDistWatershedAll,MaxProjectDist,SeedStarDistWatershedV2\n",
    "from csbdeep.models import Config, CARE\n",
    "\n",
    "from skimage.filters import threshold_local, threshold_mean, threshold_otsu\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series  # for convenience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/content/drive/My Drive/BTrack/Claudia_Green/'\n",
    "savedir = basedir + 'Denoised/'\n",
    "savesegmenteddir = savedir + '_Segmented/'\n",
    "Model_Dir = '/content/drive/My Drive/BTrack/SegmentationModel/'\n",
    "\n",
    "NoiseModelName = '3DDenoisingModel'\n",
    "UNETSegmentationModelName = 'FakeMembraneMask'\n",
    "StardistModelName = 'FakeMembraneSmartSeeds'\n",
    "\n",
    "\n",
    "max_size = 5000\n",
    "threshold = 50\n",
    "denoise = True\n",
    "\n",
    "NoiseModel = CARE(config = None, name = NoiseModelName, basedir = Model_Dir)\n",
    "model = StarDist2D(config = None, name = StardistModelName, basedir = Model_Dir)\n",
    "UnetModel = CARE(config = None, name = UNETSegmentationModelName, basedir = Model_Dir)\n",
    "Path(savedir).mkdir(exist_ok = True)\n",
    "Path(savesegmenteddir).mkdir(exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw_path = os.path.join(basedir, '*tif')\n",
    "axes = 'ZYX'\n",
    "\n",
    "if denoise:\n",
    "  filesRaw = glob.glob(Raw_path)\n",
    "  for fname in filesRaw:\n",
    "    \n",
    "          Greenimage = imread(fname)\n",
    "          \n",
    "          \n",
    "          Name = os.path.basename(os.path.splitext(fname)[0])\n",
    "          print('Denoising Image')\n",
    "          Greenimage = NoiseModel.predict(Greenimage,axes, n_tiles = (1,2,2))\n",
    "             \n",
    "          save_tiff_imagej_compatible((savedir  + Name+ '.tif' ) , Greenimage, axes)\n",
    "          print('Denoised Image saved')\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Raw_path = os.path.join(savedir, '*tif') \n",
    " \n",
    "min_size = 10 \n",
    "axis_norm = (0,1)\n",
    "axes = 'ZYX'\n",
    "saveaxes = 'ZYX'\n",
    "\n",
    "filesRaw = glob.glob(Raw_path) \n",
    "for fname in filesRaw:\n",
    "\n",
    "     print(fname) \n",
    "     #Read image        \n",
    "      image = imread(fname)\n",
    " \n",
    "      originalX = image.shape[1]\n",
    "      originalY = image.shape[2]  \n",
    "\n",
    " \n",
    "      Name = os.path.basename(os.path.splitext(fname)[0])\n",
    "      #Declare bunch of files  \n",
    "      StarsegmentationImage = np.zeros([image.shape[0], image.shape[1], image.shape[2]], dtype = ('uint16'))\n",
    "      prob = np.zeros([image.shape[0], image.shape[1], image.shape[2]], dtype = ('uint16'))\n",
    "      Watershed = np.zeros([image.shape[0], image.shape[1], image.shape[2]], dtype = ('uint16'))\n",
    "      Volume =  np.zeros([image.shape[0], image.shape[1], image.shape[2]], dtype = ('uint16')) \n",
    "      Markers = np.zeros([image.shape[0], image.shape[1], image.shape[2]], dtype = ('uint16')) \n",
    "      Finalimage = np.zeros([image.shape[0],image.shape[1],image.shape[2]], dtype = ('uint16'))\n",
    "      Segmented= np.zeros([image.shape[0],image.shape[1],image.shape[2]], dtype = ('uint16'))\n",
    "      #Loop over Z  \n",
    "      for i in range(0, image.shape[0]):\n",
    "        x = image[i,:]\n",
    " \n",
    " \n",
    "        #Make sure image is 2D\n",
    "      \n",
    "        \n",
    "        Segmented[i,:] = UnetModel.predict(x,axes, n_tiles=(8,8))\n",
    "        thresh = threshold_otsu(Segmented[i,:])\n",
    "        Finalimage[i,:] = Segmented[i,:] > thresh \n",
    "        \n",
    "\n",
    "\n",
    "        x = normalize(x,1,99.8,axis=axis_norm)\n",
    "        \n",
    "        x = zero_pad(x, 64, 64)\n",
    "        \n",
    "       \n",
    "        #Get stardist, label image, details, probability map, distance map\n",
    "        MidImage, details = model.predict_instances(x, n_tiles=(8,8))\n",
    "        StarsegmentationImage[i,:] = MidImage[:originalX, :originalY]\n",
    "        smallprob, smalldist = model.predict(x, n_tiles=(8,8))\n",
    "        grid = model.config.grid\n",
    "        midprob = cv2.resize(smallprob, dsize=(smallprob.shape[1] * grid[1] , smallprob.shape[0] * grid[0] ))\n",
    "        middist = cv2.resize(smalldist, dsize=(smalldist.shape[1] * grid[1] , smalldist.shape[0] * grid[0] ))\n",
    "        dist = MaxProjectDist(middist)\n",
    "        prob[i,:] = dist[:originalX, :originalY] \n",
    " \n",
    "\n",
    "      \n",
    " \n",
    "        #Seeds from Stardist, segmentation on probability map\n",
    "        Watershed[i,:], Markers[i,:] = SeedStarDistWatershedClaudia(prob[i,:],StarsegmentationImage[i,:].astype('uint16'),Finalimage[i,:],  model.config.grid,max_size = max_size,  min_size = min_size)    \n",
    "       \n",
    "        Watershed[i,:] = fill_label_holes(Watershed[i,:].astype('uint16'))\n",
    "        Watershed[i,:] = remove_big_objects(Watershed[i,:], max_size = max_size)\n",
    "\n",
    "\n",
    "        properties = measure.regionprops(Watershed[i,:].astype('uint16'), image[i,:])\n",
    "        Labelindex = [prop.label for prop in properties] \n",
    "        if i%2 == 0:\n",
    "          print('Zpoint', i)\n",
    "          print('Total number of Tomatos at Zpoint', i, 'is', len(Labelindex))\n",
    "          multiplot(Finalimage[i,:],prob[i,:],Markers[i,:], 'Mask', 'Distance Map', 'Markers', plotTitle = 'Segmentation Input' )\n",
    "          multiplot(x,Watershed[i,:],StarsegmentationImage[i,:].astype('uint16'), 'Original',  'SmartSeeds', 'StarDist', plotTitle = 'Super Segmentation' )\n",
    " \n",
    "      \n",
    "      Volume = merge_labels_across_volume(Watershed.astype('uint16'), RelabelZ, threshold)\n",
    "      #Save best segmentation\n",
    "    \n",
    "      save_tiff_imagej_compatible((savesegmenteddir+ 'Volume'  + Name+ '.tif' ) , Volume, saveaxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aimachine/miniconda3/envs/tensorflowEnv36/lib/python3.6/site-packages/scipy/ndimage/measurements.py:1359: RuntimeWarning: invalid value encountered in true_divide\n",
      "  for dir in range(input.ndim)]\n",
      "[INFO][2020/06/05 12:35:47 PM] btrack (v0.3.8) library imported\n",
      "[INFO][2020/06/05 12:35:47 PM] Starting BayesianTracker session\n",
      "[INFO][2020/06/05 12:35:47 PM] Loading configuration file: ./cell_config.json\n",
      "[INFO][2020/06/05 12:35:47 PM] Loading motion model: b'cell_motion'\n",
      "[INFO][2020/06/05 12:35:47 PM] Set volume to ((0, 744), (0, 700), (-100000.0, 100000.0))\n",
      "[INFO][2020/06/05 12:35:47 PM] Starting tracking... \n",
      "[INFO][2020/06/05 12:35:48 PM] Tracking objects in frames 0 to 4(of 4)...\n",
      "[INFO][2020/06/05 12:35:49 PM]  - Timing (Bayesian updates: 519.01ms, Linking: 7.36ms)\n",
      "[INFO][2020/06/05 12:35:49 PM]  - Probabilities (Link: 0.99994, Lost: 0.76746)\n",
      "[INFO][2020/06/05 12:35:49 PM] SUCCESS.\n",
      "[INFO][2020/06/05 12:35:49 PM]  - Found 1537 tracks in 4 frames (in 0.0s)\n",
      "[INFO][2020/06/05 12:35:49 PM]  - Inserted 90 dummy objects to fill tracking gaps\n",
      "[INFO][2020/06/05 12:35:49 PM] Loading hypothesis model: cell_hypothesis\n",
      "[INFO][2020/06/05 12:35:49 PM] Calculating hypotheses (relax: True)...\n",
      "[INFO][2020/06/05 12:36:13 PM] Setting up constraints matrix for global optimisation...\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
